{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-large\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.post(API_URL, headers=headers, data=data)\n",
    "    return response.json()\n",
    "\n",
    "output = query(\"C:/Users/shrey/OneDrive/Desktop/EthMumbai/artifacts/WhatsApp Image 2024-03-30 at 23.55.54_61b8d6ba.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'people sitting at tables in a large room with a ceiling'}\n"
     ]
    }
   ],
   "source": [
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    image = request.files[\"image\"]\n",
    "    image.save(\"C:/Users/shrey/OneDrive/Desktop/EthMumbai/artifacts/image1.jpg\")\n",
    "\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-large\"\n",
    "    headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "    def query(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = f.read()\n",
    "        response = requests.post(API_URL, headers=headers, data=data)\n",
    "        return response.json()\n",
    "\n",
    "    output = query(\"C:/Users/shrey/OneDrive/Desktop/EthMumbai/artifacts/image1.jpg\")\n",
    "    print(output[0][\"generated_text\"])\n",
    "\n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    # Here is my description in 5-10 words only write hinglish texts all text in english letter on\n",
    "    response = model.generate_content(\n",
    "        f\"Write a funny meme caption in 7-10 words, in the style of popular Indian internet humor, based on the following description:{output[0]['generated_text']}\"\n",
    "    )\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can't touch this :ceiling:\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-large\"\n",
    "# headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "# def query(filename):\n",
    "#     with open(filename, \"rb\") as f:\n",
    "#         data = f.read()\n",
    "#     response = requests.post(API_URL, headers=headers, data=data)\n",
    "#     return response.json()\n",
    "\n",
    "# output = query(\"C:/Users/shrey/OneDrive/Pictures/IMG_7355.JPG\")\n",
    "\n",
    "# Used to securely store your API key\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
    "genai.configure(api_key='AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk')\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "response = model.generate_content(f\"You are a Indian meme expert, you can generate short sarcastic meme captions from descriptions. Here is my description:'''{output[0]['generated_text']}'''\")\n",
    "print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"C:/Users/shrey/OneDrive/Desktop/EthMumbai/artifacts/WhatsApp Image 2024-03-30 at 23.55.54_61b8d6ba.jpg\")\n",
    "\n",
    "# Define text to be drawn\n",
    "text = response.text\n",
    "\n",
    "# Calculate font scale based on image width\n",
    "font_scale = image.shape[1] / 1000  # Adjust divisor as needed\n",
    "\n",
    "# Choose font\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Determine text size\n",
    "text_size, _ = cv2.getTextSize(text, font, font_scale, thickness=2)\n",
    "\n",
    "# Calculate text position (centered at the top)\n",
    "text_x = (image.shape[1] - text_size[0]) // 2\n",
    "text_y = text_size[1] + 50  # Adjust the value to position the text\n",
    "\n",
    "# Add white outline to the text\n",
    "cv2.putText(image, text, (text_x, text_y), font, font_scale, (255, 255, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "# Add white font to the text (thinner)\n",
    "cv2.putText(image, text, (text_x, text_y), font, font_scale, (255, 255, 255), thickness=3, lineType=cv2.LINE_AA)\n",
    "\n",
    "# Save the image with the text\n",
    "cv2.imwrite(\"output.png\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"C:/Users/shrey/OneDrive/Desktop/EthMumbai/artifacts/WhatsApp Image 2024-03-30 at 23.55.54_61b8d6ba.jpg\")\n",
    "\n",
    "# Define text to be drawn\n",
    "text = response.text\n",
    "\n",
    "# Calculate font scale based on image width\n",
    "font_scale = image.shape[1] / 800  # Adjust divisor as needed for a larger text size\n",
    "\n",
    "# Choose font\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Determine text size\n",
    "text_size, _ = cv2.getTextSize(text, font, font_scale, thickness=2)\n",
    "\n",
    "# Calculate text position (centered at the top)\n",
    "text_x = (image.shape[1] - text_size[0]) // 2\n",
    "text_y = text_size[1] + 50  # Adjust the value to position the text\n",
    "\n",
    "# Add drop shadow effect (black outline)\n",
    "shadow_offset = 2\n",
    "cv2.putText(image, text, (text_x + shadow_offset, text_y + shadow_offset), font, font_scale, (0, 0, 0), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "# Add white font to the text\n",
    "cv2.putText(image, text, (text_x, text_y), font, font_scale, (255, 255, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "# Save the image with the text\n",
    "cv2.imwrite(\"output.png\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You can't touch this :ceiling:\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"C:/Users/shrey/OneDrive/Desktop/EthMumbai/artifacts/WhatsApp Image 2024-03-30 at 23.55.54_61b8d6ba.jpg\")\n",
    "\n",
    "# Define text to be drawn\n",
    "text = response.text\n",
    "\n",
    "# Choose font scale and thickness\n",
    "font_scale = 5  # Increase the font scale for larger text\n",
    "outline_thickness = 50  # Thickness for the black outline\n",
    "font_thickness = 5\n",
    "\n",
    "# Choose text color in BGR format (here, white for the font)\n",
    "text_color = (255, 255, 255)\n",
    "\n",
    "# Choose text color in BGR format (here, black for the outline)\n",
    "outline_color = (0, 0, 0)\n",
    "\n",
    "# Load font (provide the path to your font file)\n",
    "font_path = \"path/to/ant1.ttf\"  # Replace with the actual path to your font file\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Determine text size\n",
    "text_size, _ = cv2.getTextSize(text, font, font_scale, outline_thickness)\n",
    "\n",
    "# Calculate text position (centered at the top)\n",
    "text_x = (image.shape[1] - text_size[0]) // 2\n",
    "text_y = text_size[1] + 50  # Adjust the value to position the text\n",
    "\n",
    "# Add black outline to the text\n",
    "cv2.putText(image, text, (text_x, text_y), font, font_scale, outline_color, thickness=outline_thickness)\n",
    "\n",
    "# Add white font to the text\n",
    "cv2.putText(image, text, (text_x, text_y), font, font_scale, text_color, thickness=font_thickness)\n",
    "\n",
    "# Save the image with the text\n",
    "cv2.imwrite(\"output.png\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT Crowd: Laptop squad assemble!\n"
     ]
    }
   ],
   "source": [
    "# Used to securely store your API key\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
    "genai.configure(api_key='AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk')\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "response = model.generate_content(f\"this is the description for photo '''{output[0]['generated_text']}''' write a 5- 10 word meme for it humor should be very nice\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = request.files[\"image\"]\n",
    "image_file = image.read()\n",
    "image_size = 350\n",
    "image = load_demo_image(image_file, image_size=image_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageDraw' object has no attribute 'textsize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m font \u001b[38;5;241m=\u001b[39m ImageFont\u001b[38;5;241m.\u001b[39mtruetype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marial.ttf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m36\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Determine text size\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m text_width, text_height \u001b[38;5;241m=\u001b[39m draw\u001b[38;5;241m.\u001b[39mtextsize(text, font\u001b[38;5;241m=\u001b[39mfont)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate text position (centered)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m image_width, image_height \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msize\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImageDraw' object has no attribute 'textsize'"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(\"C:/Users/shrey/OneDrive/Pictures/IMG_7355.JPG\")\n",
    "\n",
    "# Initialize the drawing context\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Define text to be drawn\n",
    "text = \"Sample Text\"\n",
    "\n",
    "# Choose a font (adjust the path to your font file as necessary)\n",
    "font = ImageFont.truetype(\"arial.ttf\", 36)\n",
    "\n",
    "# Determine text size\n",
    "text_width, text_height = draw.textsize(text, font=font)\n",
    "\n",
    "# Calculate text position (centered)\n",
    "image_width, image_height = image.size\n",
    "text_x = (image_width - text_width) // 2\n",
    "text_y = (image_height - text_height) // 2\n",
    "\n",
    "# Set text color\n",
    "text_color = (255, 255, 255)  # White\n",
    "\n",
    "# Draw text on image\n",
    "draw.text((text_x, text_y), text, fill=text_color, font=font)\n",
    "\n",
    "# Save or display the image\n",
    "image.save(\"output.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.174.47:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask_cors import CORS\n",
    "import io\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from flask import Flask, request, send_file\n",
    "import os\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "\n",
    "from PIL import ImageFont\n",
    "\n",
    "# Use a default font provided by PIL\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "# Alternatively, you can specify the font size\n",
    "font_with_size = ImageFont.load_default().font_variant(size=12)\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello_world():\n",
    "    a = \"Welcome!\"\n",
    "    return a\n",
    "\n",
    "\n",
    "@app.route(\"/upload\", methods=[\"POST\"])\n",
    "def upload():\n",
    "    # Third-party API\n",
    "    image = request.files[\"image\"]\n",
    "    image_file = image.read()\n",
    "    image_size = 350\n",
    "    image = load_demo_image(image_file, image_size=image_size, device=device)\n",
    "    image.save(\"image.jpg\")\n",
    "\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     # Beam search\n",
    "    #     # caption = model.generate(image, sample=False, num_beams=9, max_length=20, min_length=5)\n",
    "    #     # Nucleus sampling\n",
    "    #     caption = model.generate(\n",
    "    #         image, sample=True, top_p=0.9, max_length=20, min_length=5\n",
    "    #     )\n",
    "    #     print(\"caption: \" + caption[0])\n",
    "    # text = caption[0]\n",
    "    import requests\n",
    "\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-large\"\n",
    "    headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "    def query(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = f.read()\n",
    "        response = requests.post(API_URL, headers=headers, data=data)\n",
    "        return response.json()\n",
    "\n",
    "    output = query(\"image.jpg\")\n",
    "\n",
    "    # Used to securely store your API key\n",
    "    import google.generativeai as genai\n",
    "\n",
    "    # Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
    "    genai.configure(api_key='AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk')\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "    response = model.generate_content(f\"this is the description for photo '''{output[0]['generated_text']}''' write a 5- 10 word meme for it humor should be very nice\")\n",
    "    print(response.text)\n",
    "    # print(generated_text)\n",
    "\n",
    "    # Example usage\n",
    "    n = ImgText(response.text, image_file)\n",
    "    res = n.draw_text()\n",
    "    json_data = {\n",
    "        \"data\": \"https://nginx-web-fraork-njcq-uwcqowzevm.cn-chengdu.fcapp.run\" + res\n",
    "    }\n",
    "    json_img = json.dumps(json_data)\n",
    "    print(json_img)\n",
    "    return json_img\n",
    "\n",
    "\n",
    "def load_demo_image(image_file, image_size, device):\n",
    "    # Convert image to PIL Image object\n",
    "    image = Image.open(io.BytesIO(image_file))\n",
    "    # Convert image to RGB format\n",
    "    raw_image = image.convert(\"RGB\")\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                (image_size, image_size), interpolation=InterpolationMode.BICUBIC\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (0.48145466, 0.4578275, 0.40821073),\n",
    "                (0.26862954, 0.26130258, 0.27577711),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(raw_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Print tensor shape for debugging\n",
    "    print(f\"Image tensor shape: {image.shape}\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "class ImgText:\n",
    "    font = ImageFont.truetype(\"kuaikanshijieti.ttf\", 50)\n",
    "\n",
    "    def __init__(self, text, image):\n",
    "        img = Image.open(io.BytesIO(image))\n",
    "        width = img.size[0]\n",
    "        height = img.size[1]\n",
    "        print(width)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.text = text\n",
    "        self.image = image\n",
    "        self.duanluo, self.note_height, self.line_height = self.split_text()\n",
    "\n",
    "    def get_paragraph(self, text):\n",
    "        txt = Image.new(\"RGBA\", (100, 100), (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt)\n",
    "        paragraphs = \"\"\n",
    "        total_width = 0\n",
    "        line_count = 1\n",
    "        line_height = 0\n",
    "        for char in text:\n",
    "            bbox = draw.textbbox((0, 0), char, font=ImgText.font)\n",
    "            width = bbox[2] - bbox[0]\n",
    "            height = bbox[3] - bbox[1]\n",
    "            total_width += width\n",
    "            if total_width > self.width:\n",
    "                line_count += 1\n",
    "                total_width = 0\n",
    "                paragraphs += \"/n\"\n",
    "            paragraphs += char\n",
    "            line_height = max(height, line_height)\n",
    "        if not paragraphs.endswith(\"/n\"):\n",
    "            paragraphs += \"/n\"\n",
    "        return paragraphs, line_height, line_count\n",
    "\n",
    "    def split_text(self):\n",
    "        max_line_height, total_lines = 0, 0\n",
    "        allText = []\n",
    "        for text in self.text.split(\"/n\"):\n",
    "            paragraph, line_height, line_count = self.get_paragraph(text)\n",
    "            max_line_height = max(line_height, max_line_height)\n",
    "            total_lines += line_count\n",
    "            allText.append((paragraph, line_count))\n",
    "        line_height = max_line_height\n",
    "        total_height = total_lines * line_height\n",
    "        return allText, total_height, line_height\n",
    "\n",
    "    def draw_text(self):\n",
    "        note_img = Image.open(io.BytesIO(self.image)).convert(\"RGB\")\n",
    "        draw = ImageDraw.Draw(note_img)\n",
    "        x, y = 0, int(self.height) / 2\n",
    "        for paragraph, line_count in self.duanluo:\n",
    "            draw.text((x + 2, y), paragraph, fill=(0, 0, 0), font=ImgText.font)\n",
    "            draw.text((x - 2, y), paragraph, fill=(255, 255, 255), font=ImgText.font)\n",
    "            draw.text((x, y + 2), paragraph, fill=(0, 0, 0), font=ImgText.font)\n",
    "            draw.text((x, y - 2), paragraph, fill=(255, 255, 255), font=ImgText.font)\n",
    "            y += self.line_height * line_count\n",
    "\n",
    "        today = datetime.date.today()\n",
    "\n",
    "        dir_name = today.strftime(\"%Y-%m-%d\")\n",
    "        dir_path = os.path.join(\"/home/BLIP/imagedir\", dir_name)\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.mkdir(dir_path)\n",
    "        else:\n",
    "            print(\"Directory already exists, skipping creation\")\n",
    "\n",
    "        filename = str(uuid.uuid4()) + \".jpg\"\n",
    "        note_img.save(\"./imagedir/\" + dir_name + \"/\" + filename)\n",
    "        os.chmod(\"./imagedir/\" + dir_name + \"/\" + filename, 0o777)\n",
    "        return \"/imagedir/\" + dir_name + \"/\" + filename\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def callu(number):\n",
    "        headers = {\n",
    "        'Authorization': \"sk-49l0jhdx24371l57ronsc74no6o2py5ghoa6wkczwcu7xd6z6kd99lwyk3nn6hvx69\"\n",
    "        }\n",
    "\n",
    "        # Data\n",
    "        data = {\n",
    "        'phone_number': \"+91\"+ number,\n",
    "        'task': \"\"\"Fantastic! To begin your crowdfunding campaign, head to the 'Campaigns' section and follow the prompts to create your campaign page. Should you have any specific questions or need assistance, feel free to ask, and I'll be here to help.\"\n",
    "\n",
    "**Regarding participation in crowdfunding campaigns, our platform typically allows all users to participate, provided they meet certain criteria, such as compliance with our community guidelines and any legal requirements applicable to crowdfunding activities. However, it's always a good idea to review the specific terms and conditions associated with each campaign to ensure eligibility and compliance.\"\n",
    "\n",
    "User: \"Thank you for your assistance. I'll start setting up my campaign now.\"\n",
    "\"I appreciate your help. I'll reach out if I have any further questions.\"\n",
    "\"I'm impressed with the features. I'll definitely recommend this app to others.\"\n",
    "\n",
    "AI:\n",
    "\"Great to hear! If you have any further questions or need assistance in the future, don't hesitate to reach out. We're here to support you every step of the way. Thank you for choosing Dsocial's decentralized social network app, leveraging blockchain technology for secure and transparent transactions. Have a wonderful day!\n",
    "\n",
    "User: \"How secure are P2P transfers on this app? Is my financial information safe?\"\n",
    "AI: \"Security is a top priority for us. Our platform utilizes advanced encryption and security measures to ensure the safety of your financial information during P2P transfers. Additionally, we adhere to strict privacy policies to protect your personal data.\"\n",
    "\n",
    "Customer Support:\n",
    "User: \"What should I do if I encounter an issue while using the app?\"\n",
    "AI: \"If you encounter any issues or have questions while using our app, you can reach out to our dedicated customer support team for assistance. We offer various support channels, including live chat, email, and a help center, to ensure that your concerns are addressed promptly.\"\n",
    "\n",
    "Integration with External Wallets:\n",
    "User: \"Can I link my external cryptocurrency wallet to this app for P2P transfers?\"\n",
    "AI: \"At the moment, our platform supports internal wallet transactions for P2P transfers. However, we're continuously exploring options for integrating external wallets to provide more flexibility for our users. Stay tuned for any updates on this feature!\"\n",
    "\n",
    "Rewards and Loyalty Programs:\n",
    "User: \"Does the app offer any rewards or loyalty programs for frequent users?\"\n",
    "AI: \"Yes, we value our users' loyalty and engagement. We have plans to introduce rewards and loyalty programs in the near future, offering incentives for active participation and contributions within our community. Keep an eye out for announcements about these programs!\"\n",
    "\n",
    "Future Development Roadmap:\n",
    "User: \"What new features or updates can we expect from the app in the future?\"\n",
    "AI: \"We're committed to continuous improvement and innovation. Our development team is working on several exciting features and updates, including enhanced social networking capabilities, expanded payment options, and integration with decentralized finance (DeFi) protocols. We'll keep our users informed about upcoming releases and developments.\"\"\",\n",
    "        'voice_id': 1,\n",
    "        'reduce_latency': True,\n",
    "        'request_data': {},\n",
    "        'voice_settings':{\n",
    "            \"speed\": \"0.8\"\n",
    "        },\n",
    "        'interruption_threshold': 0,\n",
    "        'start_time': None,\n",
    "        'transfer_phone_number': None,\n",
    "        'answered_by_enabled': False,\n",
    "        'from': None,\n",
    "        'first_sentence': None,\n",
    "        'record': True,\n",
    "        'max_duration': 2,\n",
    "        'model': 'enhanced',\n",
    "        'language': 'ENG',\n",
    "        }\n",
    "\n",
    "        # API request \n",
    "        requests.post('https://api.bland.ai/call', json=data, headers=headers)   \n",
    "        \n",
    "        return \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callu(\"9874233126\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.getimg.ai/v1/latent-consistency/text-to-image\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"authorization\": \"Bearer key-1eJ6PrczMvKQmJVeMGr1y8p37CULdDSBAOnesUawLPqVAnE8cg8uxBl93RgLuus72xATfy7IGAiUs62gSd2QBmTwhekRiFdl\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"data\"][\"image_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "\n",
    "response = model.generate_content(\n",
    "                f\"Context provided by the user, write a hilarious meme caption in 7-10 words, in the style of popular memers, meme should be relevent to anime community.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.generativeai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_json_response\u001b[39m(prompt):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.generativeai'"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "\n",
    "def get_json_response(prompt):\n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    # Create a dictionary to hold the JSON response\n",
    "    json_response = {\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_content\": response\n",
    "    }\n",
    "    \n",
    "    return json_response\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Context provided by the user: Meme on Modi, write a hilarious meme caption in 7-10 words, in the style of popular memers, Generate a JSON response as follows: {'meme_caption': 'caption','meme_image_description':'image_describtion'}, make sure the string u are returning it should be convertable to dict object in python. and grammtically correct.\"\n",
    "json_response = get_json_response(prompt)\n",
    "result = json_response[\"generated_content\"].text\n",
    "\n",
    "# result = \"{'meme_caption': 'When you realize your crush is just as awkward as you', 'meme_image_description': ''}\"\n",
    "\n",
    "# Clean the response and ensure it is a valid JSON string\n",
    "result = result.strip().replace('```json', \"\").replace('```', \"\").replace(\"/\"\", \"\").replace('\"', \"\").replace(\"'\", \"'\").strip()\n",
    "result = result.replace(\"'\", \"/\"\")\n",
    "\n",
    "\n",
    "try:\n",
    "    json_data = json.loads(result)\n",
    "    data = {\n",
    "    \"Meme Caption\": json_data[\"meme_caption\"],\n",
    "    \"Meme Image Description\": json_data[\"meme_image_description\"]\n",
    "    }\n",
    "    print(data)\n",
    "    print(json_data)\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"Failed to decode JSON:\", e)\n",
    "    print(\"Raw response:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('{\"meme_caption\": \"Modiji ne bola tha 15 lakh aayenge, ab tak to sirf jumle hi aaye\", \"meme_image_description\": \"A photo of Modi with a text overlay that says, Modi promised 15 lakhs, but all we got are jumlas (empty promises).}',\n",
       " {'prompt': \"Context provided by the user: Meme on Modi, write a hilarious meme caption in 7-10 words, in the style of popular memers, Generate a JSON response as follows: {'meme_caption': 'caption','meme_image_description':'image_describtion'}, make sure the string u are returning it should be convertable to dict object in python. and grammtically correct.\",\n",
       "  'generated_content': response:\n",
       "  GenerateContentResponse(\n",
       "      done=True,\n",
       "      iterator=None,\n",
       "      result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '{\\'meme_caption\\': \\'Modiji ne bola tha 15 lakh aayenge, ab tak to sirf jumle hi aaye\\', \\'meme_image_description\\': \\'A photo of Modi with a text overlay that says, \"Modi promised 15 lakhs, but all we got are jumlas (empty promises).\"}'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 2, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}]}),\n",
       "  )})"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('{\\n  \"meme_caption\": \"When the teacher asks a question and you know the answer but don\"t want to seem like a nerd\",\\n  \"meme_image_description\": \"\"\\n}',\n",
       " {'meme_caption': 'Me trying to explain my logic to my cat',\n",
       "  'meme_image_description': 'A photo of a cat looking confused while a human is talking to it'})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_response = get_json_response(prompt)\n",
    "# result = json_response[\"generated_content\"].text\n",
    "# result = result.replace('```json', \"\")\n",
    "# result = result.replace('```', \"\")\n",
    "# data_string  = result.replace('/n', \"\")\n",
    "# data_string  = result.replace('\"', \"\")\n",
    "# data_string  = result.replace('*', \"\")\n",
    "\n",
    "# print(data_string)\n",
    "\n",
    "# import json\n",
    "\n",
    "# # Parse the JSON data\n",
    "# json_data = json.loads(data_string)\n",
    "\n",
    "# json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m json_data\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json_data' is not defined"
     ]
    }
   ],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 41 (char 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m data_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeme_caption\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen you realize Modi\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms beard is just a filter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeme_image_description\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA photo of Modi with a surprised expression, revealing that his beard is fake.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Parse the JSON data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m json_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(data_string)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Accessing the JSON data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeme Caption:\u001b[39m\u001b[38;5;124m\"\u001b[39m, json_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeme_caption\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 41 (char 40)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Given string containing JSON data\n",
    "data_string = '{\"meme_caption\": \"When you realize Modi\"s beard is just a filter\", \"meme_image_description\": \"A photo of Modi with a surprised expression, revealing that his beard is fake.\"}'\n",
    "# Parse the JSON data\n",
    "json_data = json.loads(data_string)\n",
    "\n",
    "# Accessing the JSON data\n",
    "print(\"Meme Caption:\", json_data[\"meme_caption\"])\n",
    "print(\"Meme Image Description:\", json_data[\"meme_image_description\"])\n",
    "\n",
    "data = {\n",
    "    \"Meme Caption:\", json_data[\"meme_caption\"],\n",
    "    \"Meme Image Description:\", json_data[\"meme_image_description\"]\n",
    "}\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virat Kohli is depicted hitting a cover drive shot, but with a comical twist, such as wearing a clown\\'s nose or holding a giant spoon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Context provided by the user: Meme on tik tok, write a hilarious meme caption in 7-10 words, in the style of popular memers, Generate a JSON response as follows: 'meme_caption': 'Your caption here','meme_image_description':'', make sure the string u are returning it should be convertable to dict object in python. \",\n",
       " 'generated_content': response:\n",
       " GenerateContentResponse(\n",
       "     done=True,\n",
       "     iterator=None,\n",
       "     result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '```json\\n{\\n  \"meme_caption\": \"When the teacher asks a question and you know the answer but don\\'t want to seem like a nerd\",\\n  \"meme_image_description\": \"\"\\n}\\n```'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 2, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}]}),\n",
       " )}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"meme_caption\": \"When the teacher asks a question and you know the answer but don\\'t want to seem like a nerd\",\\n  \"meme_image_description\": \"\"\\n}\\n```'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response[\"generated_content\"].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.generativeai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_json_response\u001b[39m(prompt):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.generativeai'"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "\n",
    "def get_json_response(prompt):\n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    # Extract JSON content from the response\n",
    "    json_response = {\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_content\": extract_json_content(response)\n",
    "    }\n",
    "    \n",
    "    return json_response\n",
    "\n",
    "def extract_json_content(response):\n",
    "    # Check if response is done and contains candidates\n",
    "    if response.done and response.result and response.result.candidates:\n",
    "        # Extract JSON content from the first candidate\n",
    "        first_candidate = response.result.candidates[0]\n",
    "        if 'content' in first_candidate and 'parts' in first_candidate['content']:\n",
    "            parts = first_candidate['content']['parts']\n",
    "            for part in parts:\n",
    "                if 'text' in part:\n",
    "                    try:\n",
    "                        # Attempt to parse text as JSON\n",
    "                        json_content = json.loads(part['text'])\n",
    "                        return json_content\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "    \n",
    "    # Return None if JSON content is not found\n",
    "    return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Context provided by the user: Virat Kholi meme , write a hilarious meme caption in 7-10 words, in the style of popular memers, Generate a JSON response as follows: 'meme_caption': 'Your caption here','meme_image_description':''\"\n",
    "json_response = get_json_response(prompt)\n",
    "# print(json.dumps(json_response, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x00000221D09753A0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 16\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(image_bytes))\n\u001b[0;32m     17\u001b[0m image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m image\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3309\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3307\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m   3308\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[1;32m-> 3309\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x00000221D09753A0>"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/ehristoforu/dalle-3-xl-v2\"\n",
    "headers = {\n",
    "            \"Authorization\": \"Bearer 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\"\n",
    "}\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.content\n",
    "\n",
    "\n",
    "image_bytes = query({\"inputs\": \"DHONI with virat kholi in a match\"})\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "image.save(\"image1.jpg\")\n",
    "image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x08\\x06\\x06\\x07\\x06\\x05\\x08\\x07\\x07\\x07\\t\\t\\x08\\n\\x0c\\x14\\r\\x0c\\x0b\\x0b\\x0c\\x19\\x12\\x13\\x0f\\x14\\x1d\\x1a\\x1f\\x1e\\x1d\\x1a\\x1c\\x1c $.\\' \",#\\x1c\\x1c(7),01444\\x1f\\'9=82<.342\\xff\\xdb\\x00C\\x01\\t\\t\\t\\x0c\\x0b\\x0c'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/fluently/Fluently-XL-Final\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        raise Exception(f\"Failed to get a valid response. Status code: {response.status_code}, Response: {response.text}\")\n",
    "\n",
    "\n",
    "image_bytes = query({\"inputs\": \"S jaishankar meme\"})\n",
    "    # Inspect the first few bytes to check if it's an image\n",
    "print(image_bytes[:100])  # Print the first 100 bytes for inspection\n",
    "# Check content type (assumes the API returns an appropriate content type header)\n",
    "\n",
    "\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "image.save(\"image1.jpg\")\n",
    "image.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting g4f\n",
      "  Downloading g4f-0.3.2.0-py3-none-any.whl.metadata (55 kB)\n",
      "     ---------------------------------------- 0.0/55.8 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/55.8 kB ? eta -:--:--\n",
      "     --------------------------- ---------- 41.0/55.8 kB 495.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 55.8/55.8 kB 584.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from g4f) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from g4f) (3.9.3)\n",
      "Requirement already satisfied: brotli in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from g4f) (1.0.9)\n",
      "Collecting pycryptodome (from g4f)\n",
      "  Downloading pycryptodome-3.20.0-cp35-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from aiohttp->g4f) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from aiohttp->g4f) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from aiohttp->g4f) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from aiohttp->g4f) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from aiohttp->g4f) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from requests->g4f) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from requests->g4f) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from requests->g4f) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from requests->g4f) (2024.2.2)\n",
      "Downloading g4f-0.3.2.0-py3-none-any.whl (603 kB)\n",
      "   ---------------------------------------- 0.0/603.5 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 92.2/603.5 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 337.9/603.5 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 603.5/603.5 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading pycryptodome-3.20.0-cp35-abi3-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.4/1.8 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.7/1.8 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.2/1.8 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.7/1.8 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 8.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pycryptodome, g4f\n",
      "Successfully installed g4f-0.3.2.0 pycryptodome-3.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install g4f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g4f.Provider.Bing supports: (\n",
      "    model: str,\n",
      "    messages: Messages,\n",
      "    proxy: str = None,\n",
      "    timeout: int = 900,\n",
      "    api_key: str = None,\n",
      "    cookies: Cookies = None,\n",
      "    tone: str = None,\n",
      "    image: ImageType = None,\n",
      "    web_search: bool = False,\n",
      "    context: str = None,\n",
      ")\n",
      "Using RetryProvider provider and gpt-3.5-turbo model\n",
      "Using ChatgptNext provider\n",
      "ChatgptNext: ClientResponseError: 429, message='Too Many Requests', url=URL('https://chat.fstha.com/api/openai/v1/chat/completions')\n",
      "Using Cnote provider\n",
      "Cnote: ClientResponseError: 404, message='Not Found', url=URL('https://p1api.xjai.pro/freeapi/chat-process')\n",
      "Using OpenaiChat provider\n",
      "OpenaiChat: MissingAuthError: Add a \"api_key\" or a .har file\n",
      "Using Aichatos provider\n",
      "\n",
      "\n",
      "\n",
      " i am result sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是https://chat18.aichatos8.com 如需合作接口调用请联系微信chatkf123 或者前往 https://binjie09.shop 自助购买key, 认为是误封需要解封的请前往https://www.ip.cn/ 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123 或者前往 https://cat.gptxyy.cn  注册使用（可付费使用gpt4 注册可免费使用3.5）\n"
     ]
    }
   ],
   "source": [
    "import g4f\n",
    "\n",
    "g4f.debug.logging = True  # Enable debug logging\n",
    "g4f.debug.version_check = False  # Disable automatic version checking\n",
    "print(g4f.Provider.Bing.params)  # Print supported args for Bing\n",
    "\n",
    "# Using automatic a provider for the given model\n",
    "# Streamed completion\n",
    "response = g4f.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\":\"Context provided by the user: YOGI aditiyanath from up, write a hilarious meme caption in 7-10 words, in the style of popular memers, meme should be relevent to anime community.\"\n",
    "}],\n",
    "    stream=True,\n",
    ")\n",
    "result = \"\"\n",
    "for message in response:\n",
    "     result += message\n",
    "    #  print(message, flush=True, end='')\n",
    "     \n",
    "     \n",
    "print(\"\\n\\n\")\n",
    "print(\" i am result\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Context provided by the user: {prompt}, write a hilarious meme caption in 7-10 words, in the style of popular memers, meme should be relevent to anime community.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RetryProvider provider and gpt-4 model\n",
      "Using Liaobots provider\n",
      "Liaobots: ResponseStatusError: Response 500: Error\n",
      "Using Bing provider\n",
      "Bing: Retry: CaptchaChallenge: User needs to solve CAPTCHA to continue.\n",
      "Bing: Retry: CaptchaChallenge: User needs to solve CAPTCHA to continue.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Normal response\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m g4f\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mg4f\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgpt_4,\n\u001b[0;32m      4\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the perspective of Lord Krishna tell me What is peace?\u001b[39m\u001b[38;5;124m\"\u001b[39m}],\n\u001b[0;32m      5\u001b[0m )  \u001b[38;5;66;03m# Alternative model setting\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi am response\u001b[39m\u001b[38;5;124m\"\u001b[39m,response)\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\g4f\\__init__.py:68\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(model, messages, provider, stream, auth, ignored, ignore_working, ignore_stream, patch_provider, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     provider \u001b[38;5;241m=\u001b[39m patch_provider(provider)\n\u001b[0;32m     67\u001b[0m result \u001b[38;5;241m=\u001b[39m provider\u001b[38;5;241m.\u001b[39mcreate_completion(model, messages, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m result])\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\g4f\\__init__.py:68\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m     provider \u001b[38;5;241m=\u001b[39m patch_provider(provider)\n\u001b[0;32m     67\u001b[0m result \u001b[38;5;241m=\u001b[39m provider\u001b[38;5;241m.\u001b[39mcreate_completion(model, messages, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m result])\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\g4f\\providers\\retry_provider.py:56\u001b[0m, in \u001b[0;36mIterListProvider.create_completion\u001b[1;34m(self, model, messages, stream, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug\u001b[38;5;241m.\u001b[39mlogging:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m provider\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m provider\u001b[38;5;241m.\u001b[39mcreate_completion(model, messages, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m token\n\u001b[0;32m     58\u001b[0m     started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\g4f\\providers\\base_provider.py:223\u001b[0m, in \u001b[0;36mAsyncGeneratorProvider.create_completion\u001b[1;34m(cls, model, messages, stream, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(await_callback(gen\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m))\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m     heappop(scheduled)\n\u001b[0;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\asyncio\\windows_events.py:444\u001b[0m, in \u001b[0;36mIocpProactor.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results:\n\u001b[1;32m--> 444\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n\u001b[0;32m    445\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\asyncio\\windows_events.py:817\u001b[0m, in \u001b[0;36mIocpProactor._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout too big\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 817\u001b[0m     status \u001b[38;5;241m=\u001b[39m _overlapped\u001b[38;5;241m.\u001b[39mGetQueuedCompletionStatus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iocp, ms)\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    819\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Normal response\n",
    "response = g4f.ChatCompletion.create(\n",
    "    model=g4f.models.gpt_4,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"In the perspective of Lord Krishna tell me What is peace?\"}],\n",
    ")  # Alternative model setting\n",
    "print(\"i am response\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RetryProvider provider and gpt-3.5-turbo model\n",
      "Using Aichatos provider\n",
      "sorry, 您的ip已由于触发防滥用检测而被封禁,可能是因为使用云服务器如腾讯云或者国外代理(vpn)访问本网站，如果使用了vpn，关闭vpn或代理即可继续使用,本服务网址是https://chat18.aichatos8.com 如需合作接口调用请联系微信chatkf123 或者前往 https://binjie09.shop 自助购买key, 认为是误封需要解封的请前往https://www.ip.cn/ 查询ip信息,并发送信息至邮件 gpt33@binjie.site ，站长会定期看邮件并处理解封和合作问题，如需调用接口请见接口文档https://apifox.com/apidoc/shared-803d9df6-a071-4b3e-9d69-ea1281614d82，如需合作接口调用请联系微信chatkf123 或者前往 https://cat.gptxyy.cn  注册使用（可付费使用gpt4 注册可免费使用3.5）\n"
     ]
    }
   ],
   "source": [
    "from g4f.client import Client\n",
    "\n",
    "client = Client()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Context provided by the user: Three 20 years boys from mumbai came to bengalore for a hckathon and the hackathon got postponded for a week, make it homurious funny, blockchain developer, designer and a python developer make it so funny, so that the people can laugh on it and the situation, write a hilarious meme caption in 7-10 words, in the style of popular Indian  memers\"}],\n",
    "\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shuttleai\n",
      "  Downloading shuttleai-4.0.9-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiofiles<24.0.0,>=23.2.1 (from shuttleai)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.9.5 (from shuttleai)\n",
      "  Downloading aiohttp-3.9.5-cp311-cp311-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting httpx<1,>=0.25.2 (from shuttleai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting matplotlib<4.0.0,>=3.9.0 (from shuttleai)\n",
      "  Downloading matplotlib-3.9.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting orjson<4.0.0,>=3.10.3 (from shuttleai)\n",
      "  Downloading orjson-3.10.3-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.9/50.9 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting poetry-version-plugin<0.3.0,>=0.2.0 (from shuttleai)\n",
      "  Downloading poetry_version_plugin-0.2.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.3 (from shuttleai)\n",
      "  Downloading pydantic-2.7.3-py3-none-any.whl.metadata (108 kB)\n",
      "     ---------------------------------------- 0.0/109.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 109.0/109.0 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting pyreadline3<4.0.0,>=3.4.1 (from shuttleai)\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.9.0.post0 (from shuttleai)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting types-aiofiles<24.0.0.0,>=23.2.0.20240403 (from shuttleai)\n",
      "  Downloading types_aiofiles-23.2.0.20240403-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.5->shuttleai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.5->shuttleai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.5->shuttleai) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.5->shuttleai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.5->shuttleai) (1.9.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25.2->shuttleai) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25.2->shuttleai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.25.2->shuttleai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25.2->shuttleai) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25.2->shuttleai) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.25.2->shuttleai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.0->shuttleai) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.0->shuttleai) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.0->shuttleai) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.0->shuttleai) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.0->shuttleai) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.0->shuttleai) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.0->shuttleai) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.9.0->shuttleai) (3.0.9)\n",
      "Collecting poetry<2.0.0,>=1.2.0 (from poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading poetry-1.8.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3.0.0,>=2.7.3->shuttleai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3.0.0,>=2.7.3->shuttleai)\n",
      "  Downloading pydantic_core-2.18.4-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.3->shuttleai) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.9.0.post0->shuttleai) (1.16.0)\n",
      "Collecting build<2.0.0,>=1.0.3 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting cachecontrol<0.15.0,>=0.14.0 (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading cachecontrol-0.14.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting cleo<3.0.0,>=2.1.0 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading cleo-2.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting crashtest<0.5.0,>=0.4.1 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading crashtest-0.4.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting dulwich<0.22.0,>=0.21.2 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading dulwich-0.21.7-cp311-cp311-win_amd64.whl.metadata (4.4 kB)\n",
      "Collecting fastjsonschema<3.0.0,>=2.18.0 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading fastjsonschema-2.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting installer<0.8.0,>=0.7.0 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading installer-0.7.0-py3-none-any.whl.metadata (936 bytes)\n",
      "Collecting keyring<25.0.0,>=24.0.0 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading keyring-24.3.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (4.8.0)\n",
      "Collecting pkginfo<2.0,>=1.10 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading pkginfo-1.11.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.0.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (3.10.0)\n",
      "Collecting poetry-core==1.9.0 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading poetry_core-1.9.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting poetry-plugin-export<2.0.0,>=1.6.0 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading poetry_plugin_export-1.8.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pyproject-hooks<2.0.0,>=1.0.0 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.26 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (1.0.0)\n",
      "Collecting shellingham<2.0,>=1.5 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting tomlkit<1.0.0,>=0.11.4 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading tomlkit-0.12.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting trove-classifiers>=2022.5.19 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading trove_classifiers-2024.5.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting virtualenv<21.0.0,>=20.23.0 (from poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading virtualenv-20.26.2-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from build<2.0.0,>=1.0.3->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (0.4.6)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from cachecontrol<0.15.0,>=0.14.0->cachecontrol[filecache]<0.15.0,>=0.14.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (1.0.3)\n",
      "Requirement already satisfied: filelock>=3.8.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (3.13.1)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from cleo<3.0.0,>=2.1.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading rapidfuzz-3.9.3-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from dulwich<0.22.0,>=0.21.2->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (2.0.7)\n",
      "Requirement already satisfied: jaraco.classes in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (3.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (7.0.1)\n",
      "Requirement already satisfied: pywin32-ctypes>=0.2.0 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from pexpect<5.0.0,>=4.7.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.26->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (2.0.4)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv<21.0.0,>=20.23.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai)\n",
      "  Downloading distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (3.17.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\shrey\\anaconda3\\lib\\site-packages (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.2.0->poetry-version-plugin<0.3.0,>=0.2.0->shuttleai) (10.1.0)\n",
      "Downloading shuttleai-4.0.9-py3-none-any.whl (29 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiohttp-3.9.5-cp311-cp311-win_amd64.whl (370 kB)\n",
      "   ---------------------------------------- 0.0/370.8 kB ? eta -:--:--\n",
      "   ---------------------------------- ---- 327.7/370.8 kB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 370.8/370.8 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.6/75.6 kB ? eta 0:00:00\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.9/77.9 kB ? eta 0:00:00\n",
      "Downloading matplotlib-3.9.0-cp311-cp311-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.7/8.0 MB 15.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.0/8.0 MB 20.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.3/8.0 MB 23.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.6/8.0 MB 24.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.7/8.0 MB 24.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/8.0 MB 24.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.0/8.0 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 23.1 MB/s eta 0:00:00\n",
      "Downloading orjson-3.10.3-cp311-none-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 138.8/138.8 kB ? eta 0:00:00\n",
      "Downloading poetry_version_plugin-0.2.0-py3-none-any.whl (8.4 kB)\n",
      "Downloading pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 0.0/409.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 409.6/409.6 kB 12.9 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.4-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.7/1.9 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 20.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 20.2 MB/s eta 0:00:00\n",
      "Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "   ---------------------------------------- 0.0/95.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 95.2/95.2 kB ? eta 0:00:00\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "   ---------------------------------------- 0.0/229.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 229.9/229.9 kB 14.6 MB/s eta 0:00:00\n",
      "Downloading types_aiofiles-23.2.0.20240403-py3-none-any.whl (9.4 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading poetry-1.8.3-py3-none-any.whl (249 kB)\n",
      "   ---------------------------------------- 0.0/249.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 249.9/249.9 kB 16.0 MB/s eta 0:00:00\n",
      "Downloading poetry_core-1.9.0-py3-none-any.whl (309 kB)\n",
      "   ---------------------------------------- 0.0/309.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 309.5/309.5 kB 18.7 MB/s eta 0:00:00\n",
      "Downloading build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading cachecontrol-0.14.0-py3-none-any.whl (22 kB)\n",
      "Downloading cleo-2.1.0-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.7/78.7 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading dulwich-0.21.7-cp311-cp311-win_amd64.whl (487 kB)\n",
      "   ---------------------------------------- 0.0/487.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 487.5/487.5 kB 15.4 MB/s eta 0:00:00\n",
      "Downloading fastjsonschema-2.19.1-py3-none-any.whl (23 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Downloading installer-0.7.0-py3-none-any.whl (453 kB)\n",
      "   ---------------------------------------- 0.0/453.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 453.8/453.8 kB 27.7 MB/s eta 0:00:00\n",
      "Downloading keyring-24.3.1-py3-none-any.whl (38 kB)\n",
      "Downloading pkginfo-1.11.1-py3-none-any.whl (31 kB)\n",
      "Downloading poetry_plugin_export-1.8.0-py3-none-any.whl (10 kB)\n",
      "Downloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading tomlkit-0.12.5-py3-none-any.whl (37 kB)\n",
      "Downloading trove_classifiers-2024.5.22-py3-none-any.whl (13 kB)\n",
      "Downloading virtualenv-20.26.2-py3-none-any.whl (3.9 MB)\n",
      "   ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.6/3.9 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.1/3.9 MB 27.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.5/3.9 MB 28.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.9/3.9 MB 25.0 MB/s eta 0:00:00\n",
      "Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
      "   ---------------------------------------- 0.0/468.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 468.9/468.9 kB 30.6 MB/s eta 0:00:00\n",
      "Downloading rapidfuzz-3.9.3-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.5/1.7 MB 46.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 34.9 MB/s eta 0:00:00\n",
      "Installing collected packages: trove-classifiers, pyreadline3, fastjsonschema, distlib, virtualenv, types-aiofiles, tomlkit, shellingham, rapidfuzz, python-dateutil, pyproject-hooks, pydantic-core, poetry-core, pkginfo, orjson, installer, h11, dulwich, crashtest, annotated-types, aiofiles, pydantic, matplotlib, keyring, httpcore, cleo, cachecontrol, build, aiohttp, httpx, poetry-plugin-export, poetry, poetry-version-plugin, shuttleai\n",
      "  Attempting uninstall: fastjsonschema\n",
      "    Found existing installation: fastjsonschema 2.16.2\n",
      "    Uninstalling fastjsonschema-2.16.2:\n",
      "      Successfully uninstalled fastjsonschema-2.16.2\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.11.1\n",
      "    Uninstalling tomlkit-0.11.1:\n",
      "      Successfully uninstalled tomlkit-0.11.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: pkginfo\n",
      "    Found existing installation: pkginfo 1.9.6\n",
      "    Uninstalling pkginfo-1.9.6:\n",
      "      Successfully uninstalled pkginfo-1.9.6\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.12\n",
      "    Uninstalling pydantic-1.10.12:\n",
      "      Successfully uninstalled pydantic-1.10.12\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.0\n",
      "    Uninstalling matplotlib-3.8.0:\n",
      "      Successfully uninstalled matplotlib-3.8.0\n",
      "  Attempting uninstall: keyring\n",
      "    Found existing installation: keyring 23.13.1\n",
      "    Uninstalling keyring-23.13.1:\n",
      "      Successfully uninstalled keyring-23.13.1\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.3\n",
      "    Uninstalling aiohttp-3.9.3:\n",
      "      Successfully uninstalled aiohttp-3.9.3\n",
      "Successfully installed aiofiles-23.2.1 aiohttp-3.9.5 annotated-types-0.7.0 build-1.2.1 cachecontrol-0.14.0 cleo-2.1.0 crashtest-0.4.1 distlib-0.3.8 dulwich-0.21.7 fastjsonschema-2.19.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 installer-0.7.0 keyring-24.3.1 matplotlib-3.9.0 orjson-3.10.3 pkginfo-1.11.1 poetry-1.8.3 poetry-core-1.9.0 poetry-plugin-export-1.8.0 poetry-version-plugin-0.2.0 pydantic-2.7.3 pydantic-core-2.18.4 pyproject-hooks-1.1.0 pyreadline3-3.4.1 python-dateutil-2.9.0.post0 rapidfuzz-3.9.3 shellingham-1.5.4 shuttleai-4.0.9 tomlkit-0.12.5 trove-classifiers-2024.5.22 types-aiofiles-23.2.0.20240403 virtualenv-20.26.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\~iohttp'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.7.3 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires python-dateutil==2.8.2, but you have python-dateutil 2.9.0.post0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install -U shuttleai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Shuttleclient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshuttleai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 3\u001b[0m shuttle \u001b[38;5;241m=\u001b[39m Shuttleclient()\n\u001b[0;32m      5\u001b[0m response \u001b[38;5;241m=\u001b[39m shuttle\u001b[38;5;241m.\u001b[39mchat_completion(\n\u001b[0;32m      6\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m             messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite me a short story about bees\u001b[39m\u001b[38;5;124m\"\u001b[39m}],\n\u001b[0;32m      8\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m          )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Shuttleclient' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from shuttleai import *\n",
    "\n",
    "shuttle = Shuttleclient()\n",
    "\n",
    "response = shuttle.chat_completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\":\"user\",\"content\":\"write me a short story about bees\"}],\n",
    "            stream=False\n",
    "         )\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ShuttleAsyncClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 72\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Embeddings Example\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;66;03m# response = await shuttle.embeddings(\u001b[39;00m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m#     model='text-embedding-ada-002',\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;66;03m#     input=\"Hello there world\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;66;03m# print(response)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m----> 8\u001b[0m      \u001b[38;5;28;01mwith\u001b[39;00m ShuttleAsyncClient(SHUTTLE_KEY, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m shuttle:\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Optionally change base url\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# shuttle.base_url = \"https://api.shuttleai.app/v1\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ShuttleAsyncClient' is not defined"
     ]
    }
   ],
   "source": [
    "from shuttleai import *\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "SHUTTLE_KEY = \"shuttle-28ffe6604ab1453db0c9\"\n",
    "\n",
    "def main():\n",
    "     with ShuttleAsyncClient(SHUTTLE_KEY, timeout=60) as shuttle:\n",
    "        \"\"\"Optionally change base url\"\"\"\n",
    "        # shuttle.base_url = \"https://api.shuttleai.app/v1\"\n",
    "\n",
    "        \"\"\"Get Models Example\"\"\"\n",
    "        # response = await shuttle.get_models()\n",
    "        # print(response)\n",
    "        \"\"\"Get Model Example\"\"\"\n",
    "        # response = await shuttle.get_model(\"gpt-4\")\n",
    "        # print(response)\n",
    "        \"\"\"Streaming Example\"\"\"\n",
    "        response =  shuttle.chat_completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=\"write me a short story about bees\",\n",
    "            stream=True,\n",
    "            plain=True,\n",
    "            internet=False\n",
    "        )\n",
    "        for chunk in response:\n",
    "            print(chunk.choices[0].delta.content)\n",
    "        \"\"\"Non-Streaming Example\"\"\"\n",
    "        # response = await shuttle.chat_completion(\n",
    "        #     model=\"gpt-3.5-turbo\",\n",
    "        #     messages=[{\"role\":\"user\",\"content\":\"write me a short story about bees\"}],\n",
    "        #     stream=False,\n",
    "        #     plain=False,\n",
    "        #     internet=False,\n",
    "        #     max_tokens=100,\n",
    "        #     temperature=0.5,\n",
    "        #     image=\"https://www.google.com/images/branding/googlelogo/1x/googlelogo_color_272x92dp.png\"\n",
    "        # )\n",
    "        # print(response.choices[0].message.content)\n",
    "        \"\"\"Image Generation Example\"\"\"\n",
    "        # response = await shuttle.images_generations(\n",
    "        #     model='sdxl',\n",
    "        #     prompt='a cute cat',\n",
    "        #     n=1,\n",
    "        # )\n",
    "        # print(response.data[0].url)\n",
    "        \"\"\"Audio Generation Example\"\"\"\n",
    "        # response = await shuttle.audio_generations(\n",
    "        #     model='eleven-labs-999',\n",
    "        #     input='Once upon a time, there was a cute cat wondering through a dark, cold forest.',\n",
    "        #     voice=\"mimi\"\n",
    "        # )\n",
    "        # print(response)\n",
    "        \"\"\"Audio Transcription Example\"\"\"\n",
    "        # response = await shuttle.audio_transcriptions(\n",
    "        #     model='whisper-large',\n",
    "        #     file=\"test.mp3\"\n",
    "        # )\n",
    "        # print(response)\n",
    "        \"\"\"Moderation Example\"\"\"\n",
    "        # response = await shuttle.moderations(\n",
    "        #     model='text-moderation-latest',\n",
    "        #     input=\"I hate you\"\n",
    "        # )\n",
    "        # print(response)\n",
    "        \"\"\"Embeddings Example\"\"\"\n",
    "        # response = await shuttle.embeddings(\n",
    "        #     model='text-embedding-ada-002',\n",
    "        #     input=\"Hello there world\"\n",
    "        # )\n",
    "        # print(response)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**most_suitable_meme_template:**  * **It's Friday and I'm happy**\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ImageDraw' object has no attribute 'textsize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Friday and I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm happy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m font_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marial.ttf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Make sure you have a font file at this path\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m meme \u001b[38;5;241m=\u001b[39m create_meme(prompt)\n\u001b[0;32m     78\u001b[0m meme\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     79\u001b[0m meme\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_meme.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 70\u001b[0m, in \u001b[0;36mcreate_meme\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     68\u001b[0m template_path, top_text, bottom_text \u001b[38;5;241m=\u001b[39m analyze_prompt_with_gemini(prompt)\n\u001b[0;32m     69\u001b[0m img \u001b[38;5;241m=\u001b[39m load_meme_template(template_path)\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m add_text_to_image(img, top_text, bottom_text)\n",
      "Cell \u001b[1;32mIn[18], line 54\u001b[0m, in \u001b[0;36madd_text_to_image\u001b[1;34m(img, top_text, bottom_text, font_path)\u001b[0m\n\u001b[0;32m     51\u001b[0m font \u001b[38;5;241m=\u001b[39m ImageFont\u001b[38;5;241m.\u001b[39mtruetype(font_path, font_size)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Calculate text size and position\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m top_text_size \u001b[38;5;241m=\u001b[39m draw\u001b[38;5;241m.\u001b[39mtextsize(top_text, font\u001b[38;5;241m=\u001b[39mfont)\n\u001b[0;32m     55\u001b[0m top_text_position \u001b[38;5;241m=\u001b[39m ((image_width \u001b[38;5;241m-\u001b[39m top_text_size[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     57\u001b[0m bottom_text_size \u001b[38;5;241m=\u001b[39m draw\u001b[38;5;241m.\u001b[39mtextsize(bottom_text, font\u001b[38;5;241m=\u001b[39mfont)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImageDraw' object has no attribute 'textsize'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "# Dictionary to map template names to their file paths\n",
    "template_paths = {\n",
    "    \"_I will find you and I will kill you_ Textless - Taken, Liam Neeson\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m1.jpg\",\n",
    "    \"It's a Surprise Tool That Will Helps Us Later_ - Mickey Mouse Clubhouse\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m2.jpg\",\n",
    "    # Add more templates here\n",
    "    \"default\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/Bugs Bunny's _No_.png\"\n",
    "}\n",
    "\n",
    "# Function to interact with Gemini API\n",
    "def analyze_prompt_with_gemini(prompt):\n",
    "\n",
    "    data = f\"Based on the prompt: '{prompt}', which meme template would be most suitable from the following options: {', '.join(template_paths.keys())}? Also, generate a suitable top and bottom text for the meme.\",\n",
    "\n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "\n",
    "    response = model.generate_content(data)\n",
    "    # response = requests.post(gemini_api_url, headers=headers, json=data)\n",
    "    # response_json = response.json()\n",
    "    \n",
    "    # print(response.text)\n",
    "    result = response.text.strip().split('\\n')\n",
    "    \n",
    "    template_choice = result[0].lower().replace(\" \", \"_\")\n",
    "    top_text = result[1] if len(result) > 1 else \"\"\n",
    "    bottom_text = result[2] if len(result) > 2 else \"\"\n",
    "    \n",
    "    print(template_choice, top_text, bottom_text)\n",
    "    \n",
    "    template_path = template_paths.get(template_choice, template_paths[\"default\"])\n",
    "    \n",
    "    return template_path, top_text, bottom_text\n",
    "\n",
    "# Function to load the chosen meme template\n",
    "def load_meme_template(template_path):\n",
    "    return Image.open(template_path)\n",
    "\n",
    "# Function to add text to the image\n",
    "def add_text_to_image(img, top_text, bottom_text, font_path=\"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    image_width, image_height = img.size\n",
    "\n",
    "    # Load a font\n",
    "    font_size = int(image_height * 0.1)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "    # Calculate text size and position\n",
    "    top_text_size = draw.textsize(top_text, font=font)\n",
    "    top_text_position = ((image_width - top_text_size[0]) / 2, 10)\n",
    "    \n",
    "    bottom_text_size = draw.textsize(bottom_text, font=font)\n",
    "    bottom_text_position = ((image_width - bottom_text_size[0]) / 2, image_height - bottom_text_size[1] - 10)\n",
    "\n",
    "    # Add text to image\n",
    "    draw.text(top_text_position, top_text, font=font, fill=\"white\")\n",
    "    draw.text(bottom_text_position, bottom_text, font=font, fill=\"white\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Function to create a meme based on the user's prompt\n",
    "def create_meme(prompt):\n",
    "    template_path, top_text, bottom_text = analyze_prompt_with_gemini(prompt)\n",
    "    img = load_meme_template(template_path)\n",
    "    return add_text_to_image(img, top_text, bottom_text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"it's Friday and I'm happy\"\n",
    "    font_path = \"arial.ttf\"  # Make sure you have a font file at this path\n",
    "\n",
    "    meme = create_meme(prompt)\n",
    "    meme.show()\n",
    "    meme.save(\"output_meme.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"**meme_template**:_it's_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicky mouse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m font_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Ensure you have a valid font file at this path\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m meme \u001b[38;5;241m=\u001b[39m create_meme(prompt)\n\u001b[0;32m     74\u001b[0m meme\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     75\u001b[0m meme\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_meme.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[34], line 64\u001b[0m, in \u001b[0;36mcreate_meme\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_meme\u001b[39m(prompt):\n\u001b[1;32m---> 64\u001b[0m     template_path, top_text, bottom_text \u001b[38;5;241m=\u001b[39m analyze_prompt_with_gemini(prompt)\n\u001b[0;32m     65\u001b[0m     img \u001b[38;5;241m=\u001b[39m load_meme_template(template_path)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add_text_to_image(img, top_text, bottom_text)\n",
      "Cell \u001b[1;32mIn[34], line 29\u001b[0m, in \u001b[0;36manalyze_prompt_with_gemini\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     26\u001b[0m top_text \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m bottom_text \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m template_path \u001b[38;5;241m=\u001b[39m template_paths\u001b[38;5;241m.\u001b[39mget(template_choice, template_paths[template_choice])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(template_path, top_text, bottom_text)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m template_path, top_text, bottom_text\n",
      "\u001b[1;31mKeyError\u001b[0m: \"**meme_template**:_it's_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\""
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "# Dictionary to map template names to their file paths\n",
    "template_paths = {\n",
    "    \"I will find you and I will kill you\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m1.jpg\",\n",
    "    \"It's a Surprise Tool That Will Helps Us Later Mickey Mouse Clubhouse\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m2.jpg\",\n",
    "    # Add more templates here\n",
    "    \"default\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/Bugs Bunny's _No_.png\"\n",
    "}\n",
    "\n",
    "# Function to interact with Gemini API\n",
    "def analyze_prompt_with_gemini(prompt):\n",
    "    data = f\"Based on the prompt: '{prompt}', which meme template would be most suitable from the following options: {', '.join(template_paths.keys())}? Also, generate a suitable top and bottom text for the meme.\"\n",
    "    \n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "    response = model.generate_content(data)\n",
    "    result = response.text.strip().split('\\n')\n",
    "    \n",
    "    template_choice = result[0].lower().replace(\" \", \"_\").replace(\"**most_suitable_meme_template:**\", \"\").replace(\"\\\\\", \"\")\n",
    "    \n",
    "    top_text = result[1] if len(result) > 1 else \"\"\n",
    "    bottom_text = result[2] if len(result) > 2 else \"\"\n",
    "    \n",
    "    template_path = template_paths.get(template_choice, template_paths[template_choice])\n",
    "    print(template_path, top_text, bottom_text)\n",
    "    \n",
    "    return template_path, top_text, bottom_text\n",
    "\n",
    "# Function to load the chosen meme template\n",
    "def load_meme_template(template_path):\n",
    "    return Image.open(template_path)\n",
    "\n",
    "# Function to add text to the image\n",
    "def add_text_to_image(img, top_text, bottom_text, font_path=\"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    image_width, image_height = img.size\n",
    "\n",
    "    # Load a font\n",
    "    font_size = int(image_height * 0.1)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "    # Calculate text size and position\n",
    "    top_text_bbox = draw.textbbox((0, 0), top_text, font=font)\n",
    "    top_text_width = top_text_bbox[2] - top_text_bbox[0]\n",
    "    top_text_position = ((image_width - top_text_width) / 2, 10)\n",
    "    \n",
    "    bottom_text_bbox = draw.textbbox((0, 0), bottom_text, font=font)\n",
    "    bottom_text_width = bottom_text_bbox[2] - bottom_text_bbox[0]\n",
    "    bottom_text_position = ((image_width - bottom_text_width) / 2, image_height - (bottom_text_bbox[3] - bottom_text_bbox[1]) - 10)\n",
    "\n",
    "    # Add text to image\n",
    "    draw.text(top_text_position, top_text, font=font, fill=\"white\")\n",
    "    draw.text(bottom_text_position, bottom_text, font=font, fill=\"white\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Function to create a meme based on the user's prompt\n",
    "def create_meme(prompt):\n",
    "    template_path, top_text, bottom_text = analyze_prompt_with_gemini(prompt)\n",
    "    img = load_meme_template(template_path)\n",
    "    return add_text_to_image(img, top_text, bottom_text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"micky mouse\"\n",
    "    font_path = \"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"  # Ensure you have a valid font file at this path\n",
    "\n",
    "    meme = create_meme(prompt)\n",
    "    meme.show()\n",
    "    meme.save(\"output_meme.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/shrey/OneDrive/Desktop/Memish/Templates/Bugs Bunny's _No_.png  **Top text:** Mickey Mouse is coming for you\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "# Dictionary to map template names to their file paths\n",
    "template_paths = {\n",
    "    \"i_will_find_you_and_i_will_kill_you\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m1.jpg\",\n",
    "    \"its_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m2.jpg\",\n",
    "    # Add more templates here\n",
    "    \"default\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/Bugs Bunny's _No_.png\"\n",
    "}\n",
    "\n",
    "# Function to interact with Gemini API\n",
    "def analyze_prompt_with_gemini(prompt):\n",
    "    data = f\"Based on the prompt: '{prompt}', which meme template would be most suitable from the following options: {', '.join(template_paths.keys())}? Also, generate a suitable top and bottom text for the meme.\"\n",
    "    \n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "    response = model.generate_content(data)\n",
    "    response_text = response.text.strip().split('\\n')\n",
    "    \n",
    "    template_choice = response_text[0].replace(\"**meme_template**:\", \"\").strip().lower().replace(\" \", \"_\")\n",
    "    top_text = response_text[1] if len(response_text) > 1 else \"\"\n",
    "    bottom_text = response_text[2] if len(response_text) > 2 else \"\"\n",
    "    \n",
    "    template_path = template_paths.get(template_choice, template_paths[\"default\"])\n",
    "    print(template_path, top_text, bottom_text)\n",
    "    \n",
    "    return template_path, top_text, bottom_text\n",
    "\n",
    "# Function to load the chosen meme template\n",
    "def load_meme_template(template_path):\n",
    "    return Image.open(template_path)\n",
    "\n",
    "# Function to add text to the image\n",
    "def add_text_to_image(img, top_text, bottom_text, font_path=\"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    image_width, image_height = img.size\n",
    "\n",
    "    # Load a font\n",
    "    font_size = int(image_height * 0.1)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "    # Calculate text size and position\n",
    "    top_text_bbox = draw.textbbox((0, 0), top_text, font=font)\n",
    "    top_text_width = top_text_bbox[2] - top_text_bbox[0]\n",
    "    top_text_position = ((image_width - top_text_width) / 2, 10)\n",
    "    \n",
    "    bottom_text_bbox = draw.textbbox((0, 0), bottom_text, font=font)\n",
    "    bottom_text_width = bottom_text_bbox[2] - bottom_text_bbox[0]\n",
    "    bottom_text_position = ((image_width - bottom_text_width) / 2, image_height - (bottom_text_bbox[3] - bottom_text_bbox[1]) - 10)\n",
    "\n",
    "    # Add text to image\n",
    "    draw.text(top_text_position, top_text, font=font, fill=\"white\")\n",
    "    draw.text(bottom_text_position, bottom_text, font=font, fill=\"white\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Function to create a meme based on the user's prompt\n",
    "def create_meme(prompt):\n",
    "    template_path, top_text, bottom_text = analyze_prompt_with_gemini(prompt)\n",
    "    img = load_meme_template(template_path)\n",
    "    return add_text_to_image(img, top_text, bottom_text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"micky mouse\"\n",
    "    font_path = \"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"  # Ensure you have a valid font file at this path\n",
    "\n",
    "    meme = create_meme(prompt)\n",
    "    meme.show()\n",
    "    meme.save(\"output_meme.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi will kill you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m font_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Ensure you have a valid font file at this path\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m meme \u001b[38;5;241m=\u001b[39m create_meme(prompt)\n\u001b[0;32m     77\u001b[0m meme\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     78\u001b[0m meme\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_meme.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[77], line 67\u001b[0m, in \u001b[0;36mcreate_meme\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_meme\u001b[39m(prompt):\n\u001b[1;32m---> 67\u001b[0m     template_path, top_text, bottom_text \u001b[38;5;241m=\u001b[39m analyze_prompt_with_gemini(prompt)\n\u001b[0;32m     68\u001b[0m     img \u001b[38;5;241m=\u001b[39m load_meme_template(template_path)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add_text_to_image(img, top_text, bottom_text)\n",
      "Cell \u001b[1;32mIn[77], line 22\u001b[0m, in \u001b[0;36manalyze_prompt_with_gemini\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(data)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     23\u001b[0m response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m template_choice \u001b[38;5;241m=\u001b[39m response_text[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**meme_template**:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\google\\generativeai\\types\\generation_types.py:412\u001b[0m, in \u001b[0;36mBaseGenerateContentResponse.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    410\u001b[0m parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parts:\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parts) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m parts[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid operation: The `response.text` quick accessor requires a simple (single-`Part`) text response. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis response is not simple text. Please use the `result.parts` accessor or the full \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`result.candidates[index].content.parts` lookup instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    421\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "# Dictionary to map template names to their file paths\n",
    "template_paths = {\n",
    "    \"i_will_find_you_and_i_will_kill_you\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m1.jpg\",\n",
    "    \"its_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m2.jpg\",\n",
    "    # Add more templates here\n",
    "    \"default\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/Bugs Bunny's _No_.png\"\n",
    "}\n",
    "\n",
    "# Function to interact with Gemini API\n",
    "def analyze_prompt_with_gemini(prompt):\n",
    "    data = f\"Based on the prompt: '{prompt}', which meme template would be most suitable from the following options: {', '.join(template_paths.keys())}? Also, generate a suitable top and bottom text for the meme., and make sure the answer u are givinf the key name should be **meme_template**\"\n",
    "    \n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "    response = model.generate_content(data)\n",
    "    print(response.text)\n",
    "    response_text = response.text.strip().split('\\n')\n",
    "    \n",
    "    template_choice = response_text[0].replace(\"**meme_template**:\", \"\").strip().lower().replace(\" \", \"_\")\n",
    "    top_text = response_text[1] if len(response_text) > 1 else \"\"\n",
    "    bottom_text = response_text[2] if len(response_text) > 2 else \"\"\n",
    "    \n",
    "    # Ensure template_choice matches our keys\n",
    "    template_choice = template_choice.replace(\":\", \"\").replace(\",\", \"\").replace(\"-\", \"_\").replace(\"'\", \"\").replace(\".\", \"\").replace(\"`\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"**meme_template**\", \"\")\n",
    "    print(template_paths[template_choice])\n",
    "    template_path = template_paths.get(template_choice, template_paths[template_choice])\n",
    "    print(template_path, top_text, bottom_text)\n",
    "    \n",
    "    return template_path, top_text, bottom_text\n",
    "\n",
    "# Function to load the chosen meme template\n",
    "def load_meme_template(template_path):\n",
    "    return Image.open(template_path)\n",
    "\n",
    "# Function to add text to the image\n",
    "def add_text_to_image(img, top_text, bottom_text, font_path=\"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    image_width, image_height = img.size\n",
    "\n",
    "    # Load a font\n",
    "    font_size = int(image_height * 0.1)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "    # Calculate text size and position\n",
    "    top_text_bbox = draw.textbbox((0, 0), top_text, font=font)\n",
    "    top_text_width = top_text_bbox[2] - top_text_bbox[0]\n",
    "    top_text_position = ((image_width - top_text_width) / 2, 10)\n",
    "    \n",
    "    bottom_text_bbox = draw.textbbox((0, 0), bottom_text, font=font)\n",
    "    bottom_text_width = bottom_text_bbox[2] - bottom_text_bbox[0]\n",
    "    bottom_text_position = ((image_width - bottom_text_width) / 2, image_height - (bottom_text_bbox[3] - bottom_text_bbox[1]) - 10)\n",
    "\n",
    "    # Add text to image\n",
    "    draw.text(top_text_position, top_text, font=font, fill=\"white\")\n",
    "    draw.text(bottom_text_position, bottom_text, font=font, fill=\"white\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Function to create a meme based on the user's prompt\n",
    "def create_meme(prompt):\n",
    "    template_path, top_text, bottom_text = analyze_prompt_with_gemini(prompt)\n",
    "    img = load_meme_template(template_path)\n",
    "    return add_text_to_image(img, top_text, bottom_text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"i will kill you\"\n",
    "    font_path = \"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"  # Ensure you have a valid font file at this path\n",
    "\n",
    "    meme = create_meme(prompt)\n",
    "    meme.show()\n",
    "    meme.save(\"output_meme.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"finish_reason\": \"SAFETY\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"MEDIUM\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"HIGH\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 98,\n",
      "        \"total_token_count\": 98\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi will kill you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m font_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Ensure you have a valid font file at this path\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m meme \u001b[38;5;241m=\u001b[39m create_meme(prompt)\n\u001b[0;32m     78\u001b[0m meme\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     79\u001b[0m meme\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_meme.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[79], line 68\u001b[0m, in \u001b[0;36mcreate_meme\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_meme\u001b[39m(prompt):\n\u001b[1;32m---> 68\u001b[0m     template_path, top_text, bottom_text \u001b[38;5;241m=\u001b[39m analyze_prompt_with_gemini(prompt)\n\u001b[0;32m     69\u001b[0m     img \u001b[38;5;241m=\u001b[39m load_meme_template(template_path)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add_text_to_image(img, top_text, bottom_text)\n",
      "Cell \u001b[1;32mIn[79], line 23\u001b[0m, in \u001b[0;36manalyze_prompt_with_gemini\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     21\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(data)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m---> 23\u001b[0m response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m template_choice \u001b[38;5;241m=\u001b[39m response_text[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**meme_template**:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m top_text \u001b[38;5;241m=\u001b[39m response_text[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response_text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\google\\generativeai\\types\\generation_types.py:412\u001b[0m, in \u001b[0;36mBaseGenerateContentResponse.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    410\u001b[0m parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parts:\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parts) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m parts[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid operation: The `response.text` quick accessor requires a simple (single-`Part`) text response. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis response is not simple text. Please use the `result.parts` accessor or the full \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`result.candidates[index].content.parts` lookup instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    421\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "# Dictionary to map template names to their file paths\n",
    "template_paths = {\n",
    "    \"i_will_find_you_and_i_will_kill_you\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m1.jpg\",\n",
    "    \"its_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m2.jpg\",\n",
    "    # Add more templates here\n",
    "    \"default\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/Bugs Bunny's _No_.png\"\n",
    "}\n",
    "\n",
    "# Function to interact with Gemini API\n",
    "def analyze_prompt_with_gemini(prompt):\n",
    "    data = f\"Based on the prompt: '{prompt}', which meme template would be most suitable from the following options: {', '.join(template_paths.keys())}? Also, generate a suitable top and bottom text for the meme. Make sure the answer you are giving the key name should be **meme_template**\"\n",
    "    \n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "    response = model.generate_content(data)\n",
    "    print(response)\n",
    "    response_text = response.text.strip().split('\\n')\n",
    "    \n",
    "    template_choice = response_text[0].replace(\"**meme_template**:\", \"\").strip().lower().replace(\" \", \"_\")\n",
    "    top_text = response_text[1] if len(response_text) > 1 else \"\"\n",
    "    bottom_text = response_text[2] if len(response_text) > 2 else \"\"\n",
    "    \n",
    "    # Ensure template_choice matches our keys\n",
    "    template_choice = template_choice.replace(\":\", \"\").replace(\",\", \"\").replace(\"-\", \"_\").replace(\"'\", \"\").replace(\".\", \"\").replace(\"`\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"**meme_template**\", \"\")\n",
    "    \n",
    "    # Safely get the template path\n",
    "    template_path = template_paths.get(template_choice, template_paths[\"default\"])\n",
    "    print(template_path, top_text, bottom_text)\n",
    "    \n",
    "    return template_path, top_text, bottom_text\n",
    "\n",
    "# Function to load the chosen meme template\n",
    "def load_meme_template(template_path):\n",
    "    return Image.open(template_path)\n",
    "\n",
    "# Function to add text to the image\n",
    "def add_text_to_image(img, top_text, bottom_text, font_path=\"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    image_width, image_height = img.size\n",
    "\n",
    "    # Load a font\n",
    "    font_size = int(image_height * 0.1)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "    # Calculate text size and position\n",
    "    top_text_bbox = draw.textbbox((0, 0), top_text, font=font)\n",
    "    top_text_width = top_text_bbox[2] - top_text_bbox[0]\n",
    "    top_text_position = ((image_width - top_text_width) / 2, 10)\n",
    "    \n",
    "    bottom_text_bbox = draw.textbbox((0, 0), bottom_text, font=font)\n",
    "    bottom_text_width = bottom_text_bbox[2] - bottom_text_bbox[0]\n",
    "    bottom_text_position = ((image_width - bottom_text_width) / 2, image_height - (bottom_text_bbox[3] - bottom_text_bbox[1]) - 10)\n",
    "\n",
    "    # Add text to image\n",
    "    draw.text(top_text_position, top_text, font=font, fill=\"white\")\n",
    "    draw.text(bottom_text_position, bottom_text, font=font, fill=\"white\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Function to create a meme based on the user's prompt\n",
    "def create_meme(prompt):\n",
    "    template_path, top_text, bottom_text = analyze_prompt_with_gemini(prompt)\n",
    "    img = load_meme_template(template_path)\n",
    "    return add_text_to_image(img, top_text, bottom_text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"i will kill you\"\n",
    "    font_path = \"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"  # Ensure you have a valid font file at this path\n",
    "\n",
    "    meme = create_meme(prompt)\n",
    "    meme.show()\n",
    "    meme.save(\"output_meme.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Gemini API: response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"**meme_template**: its_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\\n\\n**Top text**: I Miky Mouse\\n\\n**Bottom text**: You're in danger\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"LOW\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"LOW\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 98,\n",
      "        \"candidates_token_count\": 48,\n",
      "        \"total_token_count\": 146\n",
      "      }\n",
      "    }),\n",
      ")\n",
      "Template choice: its_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\n",
      "Top text: \n",
      "Bottom text: **Top text**: I Miky Mouse\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import google.generativeai as genai\n",
    "\n",
    "RESP = None\n",
    "\n",
    "# Dictionary to map template names to their file paths\n",
    "template_paths = {\n",
    "    \"i_will_find_you_and_i_will_kill_you\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m1.jpg\",\n",
    "    \"its_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/m2.jpg\",\n",
    "    # Add more templates here\n",
    "    \"default\": \"C:/Users/shrey/OneDrive/Desktop/Memish/Templates/Bugs Bunny's _No_.png\"\n",
    "}\n",
    "\n",
    "# Function to interact with Gemini API\n",
    "def analyze_prompt_with_gemini(prompt):\n",
    "    data = f\"Based on the prompt: '{prompt}', which meme template would be most suitable from the following options: {', '.join(template_paths.keys())}? Also, generate a suitable top and bottom text for the meme. Make sure the answer you are giving the key name should be **meme_template**\"\n",
    "    \n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "    response = model.generate_content(data)\n",
    "    global RESP\n",
    "    RESP = response\n",
    "    print(\"Response from Gemini API:\", RESP)  # Debugging line to check the response content\n",
    "    \n",
    "    response_text = response.text.strip().split('\\n')\n",
    "    \n",
    "    template_choice = response_text[0].replace(\"**meme_template**:\", \"\").strip().lower().replace(\" \", \"_\")\n",
    "    top_text = response_text[1] if len(response_text) > 1 else \"\"\n",
    "    bottom_text = response_text[2] if len(response_text) > 2 else \"\"\n",
    "    \n",
    "    # Ensure template_choice matches our keys\n",
    "    template_choice = template_choice.replace(\":\", \"\").replace(\",\", \"\").replace(\"-\", \"_\").replace(\"'\", \"\").replace(\".\", \"\").replace(\"`\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"**meme_template**\", \"\")\n",
    "    \n",
    "    # Safely get the template path\n",
    "    template_path = template_paths.get(template_choice, template_paths[\"default\"])\n",
    "    print(\"Template choice:\", template_choice)\n",
    "    print(\"Top text:\", top_text)\n",
    "    print(\"Bottom text:\", bottom_text)\n",
    "    \n",
    "    return template_path, top_text, bottom_text\n",
    "\n",
    "# Function to load the chosen meme template\n",
    "def load_meme_template(template_path):\n",
    "    return Image.open(template_path)\n",
    "\n",
    "# Function to add text to the image\n",
    "def add_text_to_image(img, top_text, bottom_text, font_path=\"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    image_width, image_height = img.size\n",
    "\n",
    "    # Load a font\n",
    "    font_size = int(image_height * 0.1)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "    # Calculate text size and position\n",
    "    top_text_bbox = draw.textbbox((0, 0), top_text, font=font)\n",
    "    top_text_width = top_text_bbox[2] - top_text_bbox[0]\n",
    "    top_text_position = ((image_width - top_text_width) / 2, 10)\n",
    "    \n",
    "    bottom_text_bbox = draw.textbbox((0, 0), bottom_text, font=font)\n",
    "    bottom_text_width = bottom_text_bbox[2] - bottom_text_bbox[0]\n",
    "    bottom_text_position = ((image_width - bottom_text_width) / 2, image_height - (bottom_text_bbox[3] - bottom_text_bbox[1]) - 10)\n",
    "\n",
    "    # Add text to image\n",
    "    draw.text(top_text_position, top_text, font=font, fill=\"white\")\n",
    "    draw.text(bottom_text_position, bottom_text, font=font, fill=\"white\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Function to create a meme based on the user's prompt\n",
    "def create_meme(prompt):\n",
    "    template_path, top_text, bottom_text = analyze_prompt_with_gemini(prompt)\n",
    "    img = load_meme_template(template_path)\n",
    "    return add_text_to_image(img, top_text, bottom_text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"i miky mouse\"\n",
    "    font_path = \"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"  # Ensure you have a valid font file at this path\n",
    "\n",
    "    meme = create_meme(prompt)\n",
    "    meme.show()\n",
    "    meme.save(\"output_meme.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"**meme_template**: its_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\\n\\n**Top text**: I Miky Mouse\\n\\n**Bottom text**: You're in danger\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"LOW\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"LOW\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 98,\n",
       "        \"candidates_token_count\": 48,\n",
       "        \"total_token_count\": 146\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['**meme_template**: happy_man', '**Top text**: India scores the winning run!', '**Bottom text**: The entire stadium erupts in deafening cheers']\n",
      " India scores the winning run!\n",
      "Template choice: happy_man\n",
      "Top text:  India scores the winning run!\n",
      "\n",
      "\n",
      "Bottom text:  The entire stadium erupts in deafening cheers\n",
      "[[[[[[[[[[[[[[[[[[[[[[[[[India scores the winning run!]]]]]]]]]]]]]]]]]]]]]]]]]\n",
      "[[[[[[[[[[[[[[[[[[[[[[[[[The entire stadium erupts in deafening cheers]]]]]]]]]]]]]]]]]]]]]]\n",
      "Top text position: (-30.0, 10)\n",
      "Bottom text position: (-214.5, 949)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import google.generativeai as genai\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the template paths dynamically\n",
    "template_paths = {\n",
    "    \"i_will_find_you_and_i_will_kill_you\": os.path.join(current_directory, \"Templates\", \"m1.jpg\"),\n",
    "    \"its_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\": os.path.join(current_directory, \"Templates\", \"m2.jpg\"),\n",
    "    \"default\": os.path.join(current_directory, \"Templates\", \"def.png\"),\n",
    "    \"happy_man\": os.path.join(current_directory, \"Templates\", \"m3.jpg\"),\n",
    "    \"sad_man\": os.path.join(current_directory, \"Templates\", \"m4.jpg\"),\n",
    "    \"waiting_skeleton\": os.path.join(current_directory, \"Templates\", \"m5.jpg\")\n",
    "}\n",
    "\n",
    "\n",
    "# Function to interact with Gemini API\n",
    "def analyze_prompt_with_gemini(prompt):\n",
    "    data = f\"Based on the prompt: '{prompt}', which meme template would be most suitable from the following options: {', '.join(template_paths.keys())}? Also, generate a suitable top and bottom text for the meme from your own don't just copy the user prompt, Make sure the answer you are giving the key name should be **meme_template** and funny as possible\"\n",
    "\n",
    "    \n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "    response = model.generate_content(data)\n",
    "    \n",
    "    response_text = response.text.strip().split('\\n')\n",
    "    response_text = [item for item in response_text if item]\n",
    "\n",
    "    \n",
    "    print(response_text)\n",
    "    \n",
    "    template_choice = response_text[0].replace(\"**meme_template**:\", \"\").strip().lower().replace(\" \", \"_\")\n",
    "    top_text = response_text[1] if len(response_text) > 1 else \"\"\n",
    "    top_text = top_text.replace(\"**Top text**:\", \"\").replace(\" **Top text**: \",\"\")\n",
    "    print(top_text)\n",
    "    bottom_text = response_text[2] if len(response_text) > 2 else \"\"\n",
    "    bottom_text = bottom_text.replace(\"**Bottom text**:\", \"\").replace(\" **Bottom text**: \",\"\").replace(\"**Top text**:\", \"\")\n",
    "    \n",
    "    # Ensure template_choice matches our keys\n",
    "    template_choice = template_choice.replace(\":\", \"\").replace(\",\", \"\").replace(\"-\", \"_\").replace(\"'\", \"\").replace(\".\", \"\").replace(\"`\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"**meme_template**\", \"\")\n",
    "    \n",
    "    # Safely get the template path\n",
    "    template_path = template_paths.get(template_choice, template_paths[template_choice])\n",
    "    template_path = template_path.replace(\"\\\\\", \"/\")\n",
    "    template_path = template_path.replace(\"/notebooks\", \"\")\n",
    "    print(\"Template choice:\", template_choice)\n",
    "    print(\"Top text:\", top_text)\n",
    "    print(\"\\n\")\n",
    "    print(\"Bottom text:\", bottom_text)\n",
    "    \n",
    "    return template_path, top_text, bottom_text\n",
    "\n",
    "# Function to load the chosen meme template\n",
    "def load_meme_template(template_path):\n",
    "    return Image.open(template_path)\n",
    "\n",
    "# Function to add text to the image\n",
    "def add_text_to_image(img, top_text, bottom_text, font_path=\"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    image_width, image_height = img.size\n",
    "\n",
    "    # Load a font\n",
    "    title_font_size = int(image_height * 0.1)\n",
    "    footer_font_size = int(image_height * 0.08)\n",
    "    title_font = ImageFont.truetype(font_path, title_font_size)\n",
    "    footer_font = ImageFont.truetype(font_path, footer_font_size)\n",
    "\n",
    "    # Calculate text size and position for the title (top text)\n",
    "    top_text_bbox = draw.textbbox((0, 0), top_text, font=title_font)\n",
    "    top_text_width = top_text_bbox[2] - top_text_bbox[0]\n",
    "    top_text_position = ((image_width - top_text_width) / 2, 10)\n",
    "    \n",
    "    # Calculate text size and position for the footer (bottom text)\n",
    "    bottom_text_bbox = draw.textbbox((0, 0), bottom_text, font=footer_font)\n",
    "    bottom_text_width = bottom_text_bbox[2] - bottom_text_bbox[0]\n",
    "    bottom_text_position = ((image_width - bottom_text_width) / 2, image_height - (bottom_text_bbox[3] - bottom_text_bbox[1]) - 10)\n",
    "\n",
    "    # Replace placeholder if present\n",
    "    top_text = top_text.replace(\"**Top text**:\", \"\").strip()\n",
    "    bottom_text = bottom_text.replace(\"**Bottom text**:\", \"\").strip()\n",
    "    bottom_text = bottom_text.replace(\"**Top Text**:\", \"\").strip()\n",
    "    bottom_text = bottom_text.replace(\"**Top Text**: \", \"\").strip()\n",
    "    print(f\"[[[[[[[[[[[[[[[[[[[[[[[[[{top_text}]]]]]]]]]]]]]]]]]]]]]]]]]\")\n",
    "    print(f\"[[[[[[[[[[[[[[[[[[[[[[[[[{bottom_text}]]]]]]]]]]]]]]]]]]]]]]\")\n",
    "\n",
    "    # Add text to image\n",
    "    draw.text(top_text_position, top_text, font=title_font, fill=\"white\")\n",
    "    draw.text(bottom_text_position, bottom_text, font=footer_font, fill=\"white\")\n",
    "    \n",
    "    # Debugging: Check if the positions are correct\n",
    "    print(\"Top text position:\", top_text_position)\n",
    "    print(\"Bottom text position:\", bottom_text_position)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# Function to create a meme based on the user's prompt\n",
    "def create_meme(prompt):\n",
    "    template_path, top_text, bottom_text = analyze_prompt_with_gemini(prompt)\n",
    "    img = load_meme_template(template_path)\n",
    "    return add_text_to_image(img, top_text, bottom_text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"india wins the match\"\n",
    "    font_path = \"C:/Users/shrey/OneDrive/Desktop/Memish/arial.ttf\"  # Ensure you have a valid font file at this path\n",
    "\n",
    "    meme = create_meme(prompt)\n",
    "    # meme.show()\n",
    "    meme.save(\"output_meme.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import google.generativeai as genai\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the template paths dynamically\n",
    "template_paths = {\n",
    "    \"i_will_find_you_and_i_will_kill_you\": os.path.join(current_directory, \"Templates\", \"m1.jpg\"),\n",
    "    \"its_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\": os.path.join(current_directory, \"Templates\", \"m2.jpg\"),\n",
    "    \"default\": os.path.join(current_directory, \"Templates\", \"def.png\"),\n",
    "    \"happy_man\": os.path.join(current_directory, \"Templates\", \"m3.jpg\"),\n",
    "    \"sad_man\": os.path.join(current_directory, \"Templates\", \"m4.jpg\"),\n",
    "    \"waiting_skeleton\": os.path.join(current_directory, \"Templates\", \"m5.jpg\")\n",
    "}\n",
    "\n",
    "# Function to interact with Gemini API\n",
    "def analyze_prompt_with_gemini(prompt):\n",
    "    data = f\"Based on the prompt: '{prompt}', which meme template would be most suitable from the following options: {', '.join(template_paths.keys())}? Also, generate a suitable top and bottom text for the meme from your own don't just copy the user prompt, Make sure the answer you are giving the key name should be **meme_template** and funny as possible\"\n",
    "\n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "    response = model.generate_content(data)\n",
    "    \n",
    "    response_text = response.text.strip().split('\\n')\n",
    "    response_text = [item for item in response_text if item]\n",
    "\n",
    "    template_choice = response_text[0].replace(\"**meme_template**:\", \"\").strip().lower().replace(\" \", \"_\")\n",
    "    top_text = response_text[1] if len(response_text) > 1 else \"\"\n",
    "    top_text = top_text.replace(\"**Top text**:\", \"\").replace(\" **Top text**: \",\"\")\n",
    "    bottom_text = response_text[2] if len(response_text) > 2 else \"\"\n",
    "    bottom_text = bottom_text.replace(\"**Bottom text**:\", \"\").replace(\" **Bottom text**: \",\"\").replace(\"**Top text**:\", \"\")\n",
    "    \n",
    "    # Ensure template_choice matches our keys\n",
    "    template_choice = template_choice.replace(\":\", \"\").replace(\",\", \"\").replace(\"-\", \"_\").replace(\"'\", \"\").replace(\".\", \"\").replace(\"`\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"**meme_template**\", \"\")\n",
    "    \n",
    "    # Safely get the template path\n",
    "    template_path = template_paths.get(template_choice, template_paths[template_choice])\n",
    "    template_path = template_path.replace(\"\\\\\", \"/\")\n",
    "    template_path = template_path.replace(\"/notebooks\", \"\")\n",
    "    \n",
    "    return template_path, top_text, bottom_text\n",
    "\n",
    "# Function to load the chosen meme template\n",
    "def load_meme_template(template_path):\n",
    "    img = cv2.imread(template_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {template_path}\")\n",
    "    return img\n",
    "\n",
    "# Function to add text to the image\n",
    "def add_text_to_image(img, top_text, bottom_text, font=cv2.FONT_HERSHEY_SIMPLEX):\n",
    "    image_height, image_width, _ = img.shape\n",
    "\n",
    "    # Function to wrap text\n",
    "    def wrap_text(text, font, font_scale, max_width):\n",
    "        words = text.split()\n",
    "        lines = []\n",
    "        while words:\n",
    "            line = ''\n",
    "            while words and cv2.getTextSize(line + words[0], font, font_scale, 1)[0][0] <= max_width:\n",
    "                line = line + (words.pop(0) + ' ')\n",
    "            lines.append(line.strip())\n",
    "        return lines\n",
    "\n",
    "    # Wrap top and bottom text\n",
    "    font_scale = 1\n",
    "    max_text_width = image_width - 20\n",
    "    wrapped_top_text = wrap_text(top_text, font, font_scale, max_text_width)\n",
    "    wrapped_bottom_text = wrap_text(bottom_text, font, font_scale, max_text_width)\n",
    "\n",
    "    # Calculate positions for top text\n",
    "    y_offset = 10\n",
    "    for line in wrapped_top_text:\n",
    "        text_size = cv2.getTextSize(line, font, font_scale, 1)[0]\n",
    "        text_x = (image_width - text_size[0]) // 2\n",
    "        text_y = y_offset + text_size[1]\n",
    "        cv2.putText(img, line, (text_x, text_y), font, font_scale, (255, 255, 255), 2, lineType=cv2.LINE_AA)\n",
    "        y_offset += text_size[1] + 10\n",
    "\n",
    "    # Calculate positions for bottom text\n",
    "    y_offset = image_height - 10\n",
    "    for line in reversed(wrapped_bottom_text):\n",
    "        text_size = cv2.getTextSize(line, font, font_scale, 1)[0]\n",
    "        text_x = (image_width - text_size[0]) // 2\n",
    "        text_y = y_offset\n",
    "        cv2.putText(img, line, (text_x, text_y), font, font_scale, (255, 255, 255), 2, lineType=cv2.LINE_AA)\n",
    "        y_offset -= text_size[1] + 10\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Function to create a meme based on the user's prompt\n",
    "def create_meme(prompt):\n",
    "    template_path, top_text, bottom_text = analyze_prompt_with_gemini(prompt)\n",
    "    img = load_meme_template(template_path)\n",
    "    return add_text_to_image(img, top_text, bottom_text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"it's Friday and I'm happy\"\n",
    "    try:\n",
    "        meme = create_meme(prompt)\n",
    "        cv2.imwrite(\"output_meme.jpg\", meme)\n",
    "        cv2.imshow(\"Meme\", meme)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portrait_of_a_man_with_a_wide_genuine_smile_wearing_a_motorcycle_helmet_close_up_wearing_a_red_jacket\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the template paths dynamically\n",
    "template_paths = {\n",
    "    \"i_will_find_you_and_i_will_kill_you\": os.path.join(current_directory, \"Templates\", \"m1.jpg\"),\n",
    "    \"its_a_surprise_tool_that_will_helps_us_later_mickey_mouse_clubhouse\": os.path.join(current_directory, \"Templates\", \"m2.jpg\"),\n",
    "    \"default\": os.path.join(current_directory, \"Templates\", \"def.png\"),\n",
    "    \"portrait_of_a_man_with_a_wide_genuine_smile_wearing_a_motorcycle_helmet_close_up_wearing_a_red_jacket\": os.path.join(current_directory, \"Templates\", \"m3.jpg\"),\n",
    "    \"sad_man\": os.path.join(current_directory, \"Templates\", \"m4.jpg\"),\n",
    "    \"waiting_skeleton\": os.path.join(current_directory, \"Templates\", \"m5.jpg\")\n",
    "}\n",
    "\n",
    "# Function to interact with Gemini API\n",
    "def analyze_prompt_with_gemini(prompt):\n",
    "    data = f\"Based on the prompt: '{prompt}', which meme template would be most suitable from the following options: {', '.join(template_paths.keys())}? Also, generate a suitable top and bottom text for the meme from your own don't just copy the user prompt, Make sure the answer you are giving the key name should be **meme_template** and funny as possible\"\n",
    "\n",
    "    genai.configure(api_key=\"AIzaSyBa5b8ZuK83ehPi52ua4Ly724ofJHTT5Zk\")\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "    response = model.generate_content(data)\n",
    "    \n",
    "    response_text = response.text.strip().split('\\n')\n",
    "    response_text = [item for item in response_text if item]\n",
    "\n",
    "    template_choice = response_text[0].replace(\"**meme_template**:\", \"\").strip().lower().replace(\" \", \"_\")\n",
    "    top_text = response_text[1] if len(response_text) > 1 else \"\"\n",
    "    top_text = top_text.replace(\"**Top text**:\", \"\").replace(\" **Top text**: \",\"\")\n",
    "    bottom_text = response_text[2] if len(response_text) > 2 else \"\"\n",
    "    bottom_text = bottom_text.replace(\"**Bottom text**:\", \"\").replace(\" **Bottom text**: \",\"\").replace(\"**Top text**:\", \"\")\n",
    "    \n",
    "    # Ensure template_choice matches our keys\n",
    "    template_choice = template_choice.replace(\":\", \"\").replace(\",\", \"\").replace(\"-\", \"_\").replace(\"'\", \"\").replace(\".\", \"\").replace(\"`\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").replace(\"**meme_template**\", \"\").strip()\n",
    "    if template_choice.startswith('_'):\n",
    "        template_choice = template_choice[1:]\n",
    "\n",
    "    print(template_choice)\n",
    "    # Safely get the template path\n",
    "    template_path = template_paths.get(template_choice, template_paths[template_choice])\n",
    "    template_path = template_path.replace(\"\\\\\", \"/\")\n",
    "    template_path = template_path.replace(\"/notebooks\", \"\")\n",
    "    \n",
    "    return template_path, top_text, bottom_text\n",
    "\n",
    "# Function to load the chosen meme template\n",
    "def load_meme_template(template_path):\n",
    "    img = cv2.imread(template_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {template_path}\")\n",
    "    return img\n",
    "\n",
    "# Function to add text to the image\n",
    "def add_text_to_image(img, top_text, bottom_text, font=cv2.FONT_HERSHEY_SIMPLEX):\n",
    "    image_height, image_width, _ = img.shape\n",
    "\n",
    "    # Function to wrap text\n",
    "    def wrap_text(text, font, font_scale, max_width):\n",
    "        words = text.split()\n",
    "        lines = []\n",
    "        while words:\n",
    "            line = ''\n",
    "            while words and cv2.getTextSize(line + words[0], font, font_scale, 1)[0][0] <= max_width:\n",
    "                line = line + (words.pop(0) + ' ')\n",
    "            lines.append(line.strip())\n",
    "        return lines\n",
    "\n",
    "    # Wrap top and bottom text\n",
    "    font_scale = 1\n",
    "    max_text_width = image_width - 20\n",
    "    wrapped_top_text = wrap_text(top_text, font, font_scale, max_text_width)\n",
    "    wrapped_bottom_text = wrap_text(bottom_text, font, font_scale, max_text_width)\n",
    "\n",
    "    # Calculate positions for top text\n",
    "    y_offset = 10\n",
    "    for line in wrapped_top_text:\n",
    "        text_size = cv2.getTextSize(line, font, font_scale, 1)[0]\n",
    "        text_x = (image_width - text_size[0]) // 2\n",
    "        text_y = y_offset + text_size[1]\n",
    "        cv2.putText(img, line, (text_x, text_y), font, font_scale, (255, 255, 255), 2, lineType=cv2.LINE_AA)\n",
    "        y_offset += text_size[1] + 10\n",
    "\n",
    "    # Calculate positions for bottom text\n",
    "    y_offset = image_height - 10\n",
    "    for line in reversed(wrapped_bottom_text):\n",
    "        text_size = cv2.getTextSize(line, font, font_scale, 1)[0]\n",
    "        text_x = (image_width - text_size[0]) // 2\n",
    "        text_y = y_offset\n",
    "        cv2.putText(img, line, (text_x, text_y), font, font_scale, (255, 255, 255), 2, lineType=cv2.LINE_AA)\n",
    "        y_offset -= text_size[1] + 10\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Function to create a meme based on the user's prompt\n",
    "def create_meme(prompt):\n",
    "    template_path, top_text, bottom_text = analyze_prompt_with_gemini(prompt)\n",
    "    img = load_meme_template(template_path)\n",
    "    return add_text_to_image(img, top_text, bottom_text)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"We got investment\"\n",
    "    try:\n",
    "        meme = create_meme(prompt)\n",
    "        cv2.imwrite(\"output_meme.jpg\", meme)\n",
    "        cv2.imshow(\"Meme\", meme)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portrait_of_a_man_with_a_wide_genuine_smile_wearing_a_motorcycle_helmet_close_up_wearing_a_red_jacket\n"
     ]
    }
   ],
   "source": [
    "string = \"_portrait_of_a_man_with_a_wide_genuine_smile_wearing_a_motorcycle_helmet_close_up_wearing_a_red_jacket\"\n",
    "\n",
    "if string.startswith('_'):\n",
    "    new_string = string[1:]\n",
    "else:\n",
    "    new_string = string\n",
    "\n",
    "print(new_string)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
